# 简介
RKLLM 的整体开发步骤主要分为 2 个部分：模型转换和板端部署运行。

1）模型转换：

在这一阶段，用户提供的 Hugging Face 格式的大语言模型将会被转换为 RKLLM 格式，
以便在 Rockchip NPU 平台上进行高效的推理。这一步骤包括：

a. 获取原始模型：1、开源的 Hugging Face 格式的大语言模型；2、自行训练得到的大语言模型，要求模型保存的结构与 Hugging Face 平台上的模型结构一致；3、GGUF 模型，目前仅支持 q4_0 和 fp16 类型模型；

b. 模型加载：通过 rkllm.load_huggingface()函数加载 huggingface 格式模型，通过rkllm.load_gguf()函数加载 GGUF 模型；

c. 模型量化配置：通过 rkllm.build() 函数构建 RKLLM 模型，在构建过程中可选择是否进行模型量化来提高模型部署在硬件上的性能，以及选择不同的优化等级和量化类型。

d. 模型导出：通过 rkllm.export_rkllm() 函数将 RKLLM 模型导出为一个.rkllm 格式文件，用于后续的部署。

2）板端部署运行：

这个阶段涵盖了模型的实际部署和运行。它通常包括以下步骤：

a. 模型初始化：加载 RKLLM 模型到 Rockchip NPU 平台，进行相应的模型参数设置来定义所需的文本生成方式，并提前定义用于接受实时推理结果的回调函数，进行推理前准备。

b. 模型推理：执行推理操作，将输入数据传递给模型并运行模型推理，用户可以通过预先定义的回调函数不断获取推理结果。
c. 模型释放：在完成推理流程后，释放模型资源，以便其他任务继续使用 NPU 的计算资源。

以上这两个步骤构成了完整的 RKLLM 开发流程，确保大语言模型能够成功转换、调试，并最终在 Rockchip NPU 上实现高效部署